{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"lKw_YQIrh-t_","executionInfo":{"status":"ok","timestamp":1678393418842,"user_tz":300,"elapsed":35289,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"10c9965a-7b9f-4c85-a439-2c5ba491f552"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.17.0-py3-none-any.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.13.1+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.5.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.17.0\n"]}],"source":["!pip install --no-cache-dir transformers sentencepiece &> /dev/null \n","!pip install datasets &> /dev/null \n","!pip install evaluate &> /dev/null\n","!pip install accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302,"referenced_widgets":["7a55b86c3d9a4bd9934b9c0e306a7aa1","28715e4b4e954c949e5b3869a7225ca2","add9556c029549b68e037cbbaa9a4fe0","475e29a1c0794da1ad260679075d4fed","f734ff9312144dd5a67c1fea2fe8b003","73aac4734d7243e5ac1ff31ec4de0a31","c0d8b379513b475fa9fe1d8ab36cf6cc","ed94d66497e5499c9b155bad07bbc0ff","e150eb9445cf496789a8bcd523a9549b","d1a69fd87ee14b5d962a9c8324455178","c818f51c30114f77885fc352a3d910fb","b9165ae47ec44a2f8737415b28c3039b","e6ecf466e4d04712a0bcc95bc4319b8f","c195588d5ac94c588698fcdc72af6ad6","5984b10c7fb44cb99cd962f3328eddb5","4b3991272ced4c76ab2d15f362e6b014","b938fd55a3224287a33f351e58c6d1fd"]},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678393418843,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"wul5kJ939bKA","outputId":"6be33cc9-4f99-42c6-b09d-73fe4fd1c603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Token is valid.\n","Your token has been saved in your configured git credential helpers (store).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2790,"status":"ok","timestamp":1678393422798,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"-yr3-u2U-oEd","outputId":"0123e8ec-a2ce-4179-9b02-d53923f8afe0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","NVIDIA A100-SXM4-40GB\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n"]}],"source":["import gc\n","import torch\n","# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"]},{"cell_type":"markdown","metadata":{"id":"b_XMCvLc9iAY"},"source":["# Load data"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1678392244528,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"JHzzmdtc9ea3","outputId":"3a8a4e13-7bd3-4bfa-d9c3-07b54b8a743a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_folder = '/content/drive/MyDrive/UNLP/unlp-2023-shared-task-main/data/'\\ngec_fluency_train_src = data_folder + 'gec-fluency/train.src.txt'\\ngec_fluency_train_tgt = data_folder + 'gec-fluency/train.tgt.txt'\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["\"\"\"\n","data_folder = '/content/drive/MyDrive/UNLP/unlp-2023-shared-task-main/data/'\n","gec_fluency_train_src = data_folder + 'gec-fluency/train.src.txt'\n","gec_fluency_train_tgt = data_folder + 'gec-fluency/train.tgt.txt'\n","\"\"\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_-dOV3x2C3rq","executionInfo":{"status":"ok","timestamp":1678393424098,"user_tz":300,"elapsed":3,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["#source_file = '/content/drive/MyDrive/datasets/errorified/450k-surzhik/incorr.txt'\n","#target_file = '/content/drive/MyDrive/datasets/errorified/450k-surzhik/corr.txt'\n","\n","\n","source_file = '/content/drive/MyDrive/UNLP/unlp-2023-shared-task-main/data/gec-fluency/train.src.txt'\n","target_file = '/content/drive/MyDrive/UNLP/unlp-2023-shared-task-main/data/gec-fluency/train.tgt.txt'\n","\n","punct_source = \"/content/drive/MyDrive/UNLP/assist-data/20k-punct-assist/source.txt\"\n","punct_target= \"/content/drive/MyDrive/UNLP/assist-data/20k-punct-assist/target.txt\"\n","\n","dilute_source = \"/content/drive/MyDrive/artem-yushko/data-artem/cleaned/borshch4.txt\"\n","dilute_target = \"/content/drive/MyDrive/artem-yushko/data-artem/cleaned/borshch4.txt\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CE7J8djWzBlH","executionInfo":{"status":"ok","timestamp":1678393425511,"user_tz":300,"elapsed":2,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["import pandas as pd\n","def read_parallel_into_df(source_file, target_file):\n","  # reading the input data\n","  with open(source_file, 'r') as f:\n","      source = [line[:-1] for line in f.readlines()]\n","\n","  with open(target_file, 'r') as f:\n","      target = [line[:-1] for line in f.readlines()]\n","  \n","  all_sentences = [[source[i], target[i]] for i in range(len(source))]\n","  df = pd.DataFrame(all_sentences)\n","  df.columns = ['source', 'target']\n","  return df"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"xo-GBNoDzrPG","executionInfo":{"status":"ok","timestamp":1678394968977,"user_tz":300,"elapsed":3,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["ua_gec_df = read_parallel_into_df(source_file, target_file)\n","#punct_df = read_parallel_into_df(punct_source, punct_target)\n","#dilute = read_parallel_into_df(dilute_source, dilute_target)\n","#dilute_curr = dilute[:100000]"]},{"cell_type":"code","source":["with open( '/content/drive/MyDrive/UNLP/unlp-2023-shared-task-main/data/gec-fluency/train.m2', 'r') as f:\n","  sentences = f.read().split('\\n\\n')[:-1]\n","\n","sentence_split = []\n","for sentence in sentences:\n","  sentence_split.append(sentence.split('\\n'))\n","\n","sent_df = []\n","for sent in sentence_split:\n","\n","  edits = []\n","  for edit in sent[1:]:\n","    edit_lst = edit.split('|||')\n","    edit_type = edit_lst[1]\n","    ref = edit_lst[-1]\n","\n","    edits.append([edit_type, ref])\n","  sent_dict = {'sent':sent[0], \n","               'all':sent, \n","               'total-edits':len(sent)-1,\n","               'all-edits':edits}\n","  sent_df.append(sent_dict)\n","sent_df = pd.DataFrame(sent_df)\n","\n","big_edits = []\n","for edits in sent_df['all-edits']:\n","  for edit in edits:\n","    big_edits.append(edit[0])\n","\n","all_cats = list(set(big_edits))\n","\n","for cat in all_cats:\n","  sent_df[cat] = 0\n","\n","sent_df['ref0'] = 0\n","sent_df['ref1'] = 0\n","\n","for i in range(len(sent_df)):\n","  edits = sent_df['all-edits'][i]\n","  for edit in edits:\n","    sent_df[edit[0]][i] += 1\n","    sent_df[f'ref{edit[1]}'][i] +=1\n","\n","drop_idx = list(sent_df[sent_df['total-edits']>6].index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yuQzVsMC0l1","executionInfo":{"status":"ok","timestamp":1678394928197,"user_tz":300,"elapsed":30713,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"95ffafc8-79bb-464a-af43-ff4a9bb7b0f4"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-32-52a0a700dd96>:41: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  sent_df[edit[0]][i] += 1\n","<ipython-input-32-52a0a700dd96>:42: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  sent_df[f'ref{edit[1]}'][i] +=1\n"]}]},{"cell_type":"code","source":["sent_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Aoy2kjuhMevP","executionInfo":{"status":"ok","timestamp":1678395996393,"user_tz":300,"elapsed":5,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"36ebd657-b333-4e7b-c1db-2e27511db068"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                    sent  \\\n","0                                               S # 0000   \n","1      S Byte for France або “ Мій досвід ведення бло...   \n","2      S Останні 3 місяці мого життя видалися аж зана...   \n","3      S Сьогодні розповім про те як і навіщо мене за...   \n","4      S Якщо цікаво подивитися відразу на результат ...   \n","...                                                  ...   \n","32729                                           S # 1889   \n","32730                 S Яким боком московити нам брати ?   \n","32731   S І навіть якщо брати , то Авель теж мав брата .   \n","32732                                           S # 1890   \n","32733  S Якщо судити по автівці , то де Папа , а де я...   \n","\n","                                                     all  total-edits  \\\n","0      [S # 0000, A -1 -1|||noop|||-NONE-|||REQUIRED|...            2   \n","1      [S Byte for France або “ Мій досвід ведення бл...            2   \n","2      [S Останні 3 місяці мого життя видалися аж зан...            7   \n","3      [S Сьогодні розповім про те як і навіщо мене з...            3   \n","4      [S Якщо цікаво подивитися відразу на результат...            3   \n","...                                                  ...          ...   \n","32729  [S # 1889, A -1 -1|||noop|||-NONE-|||REQUIRED|...            2   \n","32730  [S Яким боком московити нам брати ?, A 0 2|||F...            2   \n","32731  [S І навіть якщо брати , то Авель теж мав брат...            1   \n","32732  [S # 1890, A -1 -1|||noop|||-NONE-|||REQUIRED|...            2   \n","32733  [S Якщо судити по автівці , то де Папа , а де ...            2   \n","\n","                                               all-edits  G/Conjunction  \\\n","0                                 [[noop, 0], [noop, 1]]              0   \n","1                         [[Spelling, 0], [Spelling, 1]]              0   \n","2      [[F/Style, 0], [Punctuation, 0], [F/Style, 0],...              0   \n","3      [[Punctuation, 0], [Spelling, 0], [Punctuation...              0   \n","4      [[Punctuation, 0], [F/Style, 1], [Punctuation,...              0   \n","...                                                  ...            ...   \n","32729                             [[noop, 0], [noop, 1]]              0   \n","32730                   [[F/Style, 0], [Punctuation, 0]]              0   \n","32731                                        [[noop, 0]]              0   \n","32732                             [[noop, 0], [noop, 1]]              0   \n","32733                         [[G/Prep, 0], [G/Prep, 0]]              0   \n","\n","       G/PartVoice  G/UngrammaticalStructure  F/PoorFlow  noop  Spelling  ...  \\\n","0                0                         0           0     2         0  ...   \n","1                0                         0           0     0         2  ...   \n","2                0                         0           3     0         0  ...   \n","3                0                         0           0     0         1  ...   \n","4                0                         0           0     0         0  ...   \n","...            ...                       ...         ...   ...       ...  ...   \n","32729            0                         0           0     2         0  ...   \n","32730            0                         0           0     0         0  ...   \n","32731            0                         0           0     1         0  ...   \n","32732            0                         0           0     2         0  ...   \n","32733            0                         0           0     0         0  ...   \n","\n","       Other  G/VerbVoice  F/Collocation  F/Repetition  G/Other  F/Other  \\\n","0          0            0              0             0        0        0   \n","1          0            0              0             0        0        0   \n","2          0            0              0             0        0        0   \n","3          0            0              0             0        0        0   \n","4          0            0              0             0        0        0   \n","...      ...          ...            ...           ...      ...      ...   \n","32729      0            0              0             0        0        0   \n","32730      0            0              0             0        0        0   \n","32731      0            0              0             0        0        0   \n","32732      0            0              0             0        0        0   \n","32733      0            0              0             0        0        0   \n","\n","       G/Comparison  G/Prep  ref0  ref1  \n","0                 0       0     1     1  \n","1                 0       0     1     1  \n","2                 0       0     3     4  \n","3                 0       0     2     1  \n","4                 0       0     1     2  \n","...             ...     ...   ...   ...  \n","32729             0       0     1     1  \n","32730             0       0     2     0  \n","32731             0       0     1     0  \n","32732             0       0     1     1  \n","32733             0       2     2     0  \n","\n","[32734 rows x 30 columns]"],"text/html":["\n","  <div id=\"df-1e7baed9-29a6-4eac-8097-bc9029440f90\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent</th>\n","      <th>all</th>\n","      <th>total-edits</th>\n","      <th>all-edits</th>\n","      <th>G/Conjunction</th>\n","      <th>G/PartVoice</th>\n","      <th>G/UngrammaticalStructure</th>\n","      <th>F/PoorFlow</th>\n","      <th>noop</th>\n","      <th>Spelling</th>\n","      <th>...</th>\n","      <th>Other</th>\n","      <th>G/VerbVoice</th>\n","      <th>F/Collocation</th>\n","      <th>F/Repetition</th>\n","      <th>G/Other</th>\n","      <th>F/Other</th>\n","      <th>G/Comparison</th>\n","      <th>G/Prep</th>\n","      <th>ref0</th>\n","      <th>ref1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>S # 0000</td>\n","      <td>[S # 0000, A -1 -1|||noop|||-NONE-|||REQUIRED|...</td>\n","      <td>2</td>\n","      <td>[[noop, 0], [noop, 1]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>S Byte for France або “ Мій досвід ведення бло...</td>\n","      <td>[S Byte for France або “ Мій досвід ведення бл...</td>\n","      <td>2</td>\n","      <td>[[Spelling, 0], [Spelling, 1]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>S Останні 3 місяці мого життя видалися аж зана...</td>\n","      <td>[S Останні 3 місяці мого життя видалися аж зан...</td>\n","      <td>7</td>\n","      <td>[[F/Style, 0], [Punctuation, 0], [F/Style, 0],...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>S Сьогодні розповім про те як і навіщо мене за...</td>\n","      <td>[S Сьогодні розповім про те як і навіщо мене з...</td>\n","      <td>3</td>\n","      <td>[[Punctuation, 0], [Spelling, 0], [Punctuation...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>S Якщо цікаво подивитися відразу на результат ...</td>\n","      <td>[S Якщо цікаво подивитися відразу на результат...</td>\n","      <td>3</td>\n","      <td>[[Punctuation, 0], [F/Style, 1], [Punctuation,...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>32729</th>\n","      <td>S # 1889</td>\n","      <td>[S # 1889, A -1 -1|||noop|||-NONE-|||REQUIRED|...</td>\n","      <td>2</td>\n","      <td>[[noop, 0], [noop, 1]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32730</th>\n","      <td>S Яким боком московити нам брати ?</td>\n","      <td>[S Яким боком московити нам брати ?, A 0 2|||F...</td>\n","      <td>2</td>\n","      <td>[[F/Style, 0], [Punctuation, 0]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32731</th>\n","      <td>S І навіть якщо брати , то Авель теж мав брата .</td>\n","      <td>[S І навіть якщо брати , то Авель теж мав брат...</td>\n","      <td>1</td>\n","      <td>[[noop, 0]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>32732</th>\n","      <td>S # 1890</td>\n","      <td>[S # 1890, A -1 -1|||noop|||-NONE-|||REQUIRED|...</td>\n","      <td>2</td>\n","      <td>[[noop, 0], [noop, 1]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32733</th>\n","      <td>S Якщо судити по автівці , то де Папа , а де я...</td>\n","      <td>[S Якщо судити по автівці , то де Папа , а де ...</td>\n","      <td>2</td>\n","      <td>[[G/Prep, 0], [G/Prep, 0]]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>32734 rows × 30 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e7baed9-29a6-4eac-8097-bc9029440f90')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1e7baed9-29a6-4eac-8097-bc9029440f90 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1e7baed9-29a6-4eac-8097-bc9029440f90');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["len(drop_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HuWsZKjyDji3","executionInfo":{"status":"ok","timestamp":1678394976648,"user_tz":300,"elapsed":20,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"398c7fcb-13f8-4374-a744-69e79304e1a0"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["472"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["for id in drop_idx:\n","  ua_gec_df = ua_gec_df.drop(id, axis=0)\n","len(ua_gec_df)"],"metadata":{"id":"7ECPgn7nlo-l","executionInfo":{"status":"ok","timestamp":1678394985312,"user_tz":300,"elapsed":2,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eef610aa-f199-4676-9e40-c7b4826eda82"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32262"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":1666,"status":"ok","timestamp":1678394987746,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"aKHOSfot9sIq"},"outputs":[],"source":["train_df = pd.concat([ua_gec_df])\n","\n","# converting pandas dataframe to Dataset class\n","from datasets import Dataset\n","raw_dataset = Dataset.from_pandas(train_df)"]},{"cell_type":"code","source":["raw_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nc8FHdjXwia1","executionInfo":{"status":"ok","timestamp":1678394987747,"user_tz":300,"elapsed":5,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"2d418985-280e-44d6-bc93-6e9d466b39a0"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['source', 'target', '__index_level_0__'],\n","    num_rows: 32262\n","})"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"WNaJJ_l2icv0"}},{"cell_type":"code","execution_count":40,"metadata":{"id":"f7ARYSh8Fi5z","executionInfo":{"status":"ok","timestamp":1678394991613,"user_tz":300,"elapsed":2,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["# SET MODEL NAME\n","model_name = 'unlp-drop-f-errors'"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3408,"status":"ok","timestamp":1678394999052,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"Nz-c0SJJ9t_9","outputId":"38342a51-482c-423d-c278-61ce4deca401"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/config.json\n","Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/sentencepiece.bpe.model\n","loading file tokenizer.json from cache at None\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/config.json\n","Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/config.json\n","Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n"]}],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"facebook/mbart-large-50\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, src_lang=\"uk_UA\", tgt_lang=\"uk_UA\")"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1678394999052,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"S3Zb7YfSBzQL","outputId":"2e656180-1050-41cf-b4c1-e929828da5ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': [250048, 1813, 24046, 6, 144279, 2800, 1443, 43282, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [250048, 1813, 24046, 4, 43760, 10506, 4, 1443, 43282, 32, 2]}\n","['uk_UA', '▁При', 'віт', '▁', 'андр', 'ій', '▁як', '▁справи', '</s>']\n","['uk_UA', '▁При', 'віт', ',', '▁Андр', 'ію', ',', '▁як', '▁справи', '?', '</s>']\n"]}],"source":["ukr_sent_src = \"Привіт андрій як справи\"\n","ukr_sent_tgt = \"Привіт, Андрію, як справи?\"\n","\n","inputs = tokenizer(ukr_sent_src, text_target=ukr_sent_tgt)\n","print(inputs)\n","print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))\n","print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"GfIITHwOB0-a","executionInfo":{"status":"ok","timestamp":1678394999052,"user_tz":300,"elapsed":6,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["max_length = 128\n","\n","def preprocess_function(examples):\n","    inputs = examples[\"source\"]\n","    targets = examples[\"target\"]\n","    model_inputs = tokenizer(\n","        inputs, text_target=targets, max_length=max_length, truncation=True\n","    )\n","    return model_inputs"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["5c24e42c8e104c6fbc84bee0881bf5ef","b975b5de3a534bdd83836c875e31d1b1","5a1001a0e0974b199fd785fffae3a85c","bb69069067754791ab321a754dd22a40","d6ec8516742941b2b36086302925df25","4c33526217f94ec48878f81196f4fa0e","cf891bc3b73d4b758a433bd05ee75f3b","bfc51726ebcd4200bbdcfaa6f065ac1c","71d75514807e424582ddd83d62e9bbf8","3fdfbd63b9924c0cac865c9e25c411e1","0e0ae0cc04f0405bbc68a765eac79b86"]},"executionInfo":{"elapsed":1383,"status":"ok","timestamp":1678395000429,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"NXqEm3TjB3gu","outputId":"142e2ca2-6d0e-4c77-da5d-3dab65e3ea77"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/32262 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c24e42c8e104c6fbc84bee0881bf5ef"}},"metadata":{}}],"source":["tokenized_datasets = raw_dataset.map(\n","    preprocess_function,\n","    batched=True,\n",")"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11542,"status":"ok","timestamp":1678395011954,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"ONaXJhE4B5U_","outputId":"078f6093-2a40-4013-a5c8-539c7d6b331e"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/config.json\n","Model config MBartConfig {\n","  \"_name_or_path\": \"facebook/mbart-large-50\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"MBartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 12,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 200,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"mbart\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 5,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": false,\n","  \"tokenizer_class\": \"MBart50Tokenizer\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250054\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/pytorch_model.bin\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n","All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n","\n","All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at facebook/mbart-large-50.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--facebook--mbart-large-50/snapshots/85b0b6b66d2330ba1ad2da18dd3823f8451a929b/generation_config.json\n","Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": 2,\n","  \"early_stopping\": true,\n","  \"eos_token_id\": 2,\n","  \"forced_eos_token_id\": 2,\n","  \"max_length\": 200,\n","  \"num_beams\": 5,\n","  \"pad_token_id\": 1,\n","  \"transformers_version\": \"4.26.1\"\n","}\n","\n"]}],"source":["from transformers import AutoModelForSeq2SeqLM\n","\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"Z7I2yfAtB69g","executionInfo":{"status":"ok","timestamp":1678395013462,"user_tz":300,"elapsed":1511,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"3IcRBEM8B_Z0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678395013463,"user_tz":300,"elapsed":8,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"03ace3f3-6335-4771-96a6-b1656a92a0cb"},"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["from transformers import Seq2SeqTrainingArguments\n","\n","args = Seq2SeqTrainingArguments(\n","    model_name,\n","    evaluation_strategy=\"no\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=64,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=3,\n","    predict_with_generate=True,\n","    fp16=True,\n","    push_to_hub=True\n",")"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6912,"status":"ok","timestamp":1678395020370,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"IFroXwmPDgTk","outputId":"1f6613c9-354b-4c2d-8923-188e0f0451c3"},"outputs":[{"output_type":"stream","name":"stderr","text":["Cloning https://huggingface.co/lenguist/unlp-drop-total-edits-6up into local empty directory.\n","WARNING:huggingface_hub.repository:Cloning https://huggingface.co/lenguist/unlp-drop-total-edits-6up into local empty directory.\n","Using cuda_amp half precision backend\n"]}],"source":["from transformers import Seq2SeqTrainer\n","\n","trainer = Seq2SeqTrainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer\n",")"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"Usi4pNVN_pWm","executionInfo":{"status":"ok","timestamp":1678395020371,"user_tz":300,"elapsed":20,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}}},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":975140,"status":"ok","timestamp":1678395995493,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"},"user_tz":300},"id":"TWIhrcvYGXWW","outputId":"894b9038-fe49-4da7-ad01-03d656de79bd"},"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `MBartForConditionalGeneration.forward` and have been ignored: __index_level_0__, source, target. If __index_level_0__, source, target are not expected by `MBartForConditionalGeneration.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 32262\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3027\n","  Number of trainable parameters = 610879488\n","You're using a MBart50TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3027' max='3027' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3027/3027 16:14, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.515900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.256800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.161900</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.157800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.113700</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.101900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to unlp-drop-total-edits-6up/checkpoint-1009\n","Configuration saved in unlp-drop-total-edits-6up/checkpoint-1009/config.json\n","Configuration saved in unlp-drop-total-edits-6up/checkpoint-1009/generation_config.json\n","Model weights saved in unlp-drop-total-edits-6up/checkpoint-1009/pytorch_model.bin\n","tokenizer config file saved in unlp-drop-total-edits-6up/checkpoint-1009/tokenizer_config.json\n","Special tokens file saved in unlp-drop-total-edits-6up/checkpoint-1009/special_tokens_map.json\n","tokenizer config file saved in unlp-drop-total-edits-6up/tokenizer_config.json\n","Special tokens file saved in unlp-drop-total-edits-6up/special_tokens_map.json\n","Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n","WARNING:huggingface_hub.repository:Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n","Saving model checkpoint to unlp-drop-total-edits-6up/checkpoint-2018\n","Configuration saved in unlp-drop-total-edits-6up/checkpoint-2018/config.json\n","Configuration saved in unlp-drop-total-edits-6up/checkpoint-2018/generation_config.json\n","Model weights saved in unlp-drop-total-edits-6up/checkpoint-2018/pytorch_model.bin\n","tokenizer config file saved in unlp-drop-total-edits-6up/checkpoint-2018/tokenizer_config.json\n","Special tokens file saved in unlp-drop-total-edits-6up/checkpoint-2018/special_tokens_map.json\n","tokenizer config file saved in unlp-drop-total-edits-6up/tokenizer_config.json\n","Special tokens file saved in unlp-drop-total-edits-6up/special_tokens_map.json\n","Saving model checkpoint to unlp-drop-total-edits-6up/checkpoint-3027\n","Configuration saved in unlp-drop-total-edits-6up/checkpoint-3027/config.json\n","Configuration saved in unlp-drop-total-edits-6up/checkpoint-3027/generation_config.json\n","Model weights saved in unlp-drop-total-edits-6up/checkpoint-3027/pytorch_model.bin\n","tokenizer config file saved in unlp-drop-total-edits-6up/checkpoint-3027/tokenizer_config.json\n","Special tokens file saved in unlp-drop-total-edits-6up/checkpoint-3027/special_tokens_map.json\n","tokenizer config file saved in unlp-drop-total-edits-6up/tokenizer_config.json\n","Special tokens file saved in unlp-drop-total-edits-6up/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3027, training_loss=0.21696207150792304, metrics={'train_runtime': 974.8387, 'train_samples_per_second': 99.284, 'train_steps_per_second': 3.105, 'total_flos': 1.4565393452531712e+16, 'train_loss': 0.21696207150792304, 'epoch': 3.0})"]},"metadata":{},"execution_count":50}],"source":["trainer.train()"]},{"cell_type":"markdown","source":["#MT5 experiments"],"metadata":{"id":"dZNdwUwNibAL"}},{"cell_type":"code","source":["from transformers import MT5ForConditionalGeneration, AutoTokenizer\n","\n","model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n","tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CkDyBwRwid6J","executionInfo":{"status":"ok","timestamp":1677966096537,"user_tz":300,"elapsed":7852,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"a170815a-2d88-4270-bfe6-aa264950d888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["text_src = \"Привіт світ\"\n","text_tgt = \"Привіт світe\"\n","inputs = tokenizer(text_src, text_target=text_tgt, return_tensors=\"pt\")\n","\n","outputs = model(**inputs)\n","loss = outputs.loss"],"metadata":{"id":"oEmV-YIHksWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","translator = pipeline(\"text-generation\", model= \"google/mt5-base\", device=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wn1kkOvZlJNi","executionInfo":{"status":"ok","timestamp":1677966347684,"user_tz":300,"elapsed":11642,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"8456c1ed-739c-4574-a3f4-d5116fb63bf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","The model 'MT5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel'].\n"]}]},{"cell_type":"code","source":["translator(\"Привіт світ\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSLukr62lboc","executionInfo":{"status":"ok","timestamp":1677966381608,"user_tz":300,"elapsed":508,"user":{"displayName":"Ukramarly","userId":"12331974385466066517"}},"outputId":"fa658b5a-b3cf-4894-cad7-affb33835ddd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'Привіт світ>... <extra_id_1>? <extra_id_1>? <extra_id_29>lêdobitlivezaítвид Iвидi...'}]"]},"metadata":{},"execution_count":34}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["dZNdwUwNibAL"],"mount_file_id":"1HoRDqPjhNxer926wrxLQPBUENZgE2_QG","authorship_tag":"ABX9TyNFXmqcK64RZenqNQcwoZX2"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7a55b86c3d9a4bd9934b9c0e306a7aa1":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_28715e4b4e954c949e5b3869a7225ca2","IPY_MODEL_add9556c029549b68e037cbbaa9a4fe0","IPY_MODEL_475e29a1c0794da1ad260679075d4fed","IPY_MODEL_f734ff9312144dd5a67c1fea2fe8b003","IPY_MODEL_73aac4734d7243e5ac1ff31ec4de0a31"],"layout":"IPY_MODEL_c0d8b379513b475fa9fe1d8ab36cf6cc"}},"28715e4b4e954c949e5b3869a7225ca2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed94d66497e5499c9b155bad07bbc0ff","placeholder":"​","style":"IPY_MODEL_e150eb9445cf496789a8bcd523a9549b","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"add9556c029549b68e037cbbaa9a4fe0":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_d1a69fd87ee14b5d962a9c8324455178","placeholder":"​","style":"IPY_MODEL_c818f51c30114f77885fc352a3d910fb","value":""}},"475e29a1c0794da1ad260679075d4fed":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_b9165ae47ec44a2f8737415b28c3039b","style":"IPY_MODEL_e6ecf466e4d04712a0bcc95bc4319b8f","value":true}},"f734ff9312144dd5a67c1fea2fe8b003":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_c195588d5ac94c588698fcdc72af6ad6","style":"IPY_MODEL_5984b10c7fb44cb99cd962f3328eddb5","tooltip":""}},"73aac4734d7243e5ac1ff31ec4de0a31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b3991272ced4c76ab2d15f362e6b014","placeholder":"​","style":"IPY_MODEL_b938fd55a3224287a33f351e58c6d1fd","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"c0d8b379513b475fa9fe1d8ab36cf6cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"ed94d66497e5499c9b155bad07bbc0ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e150eb9445cf496789a8bcd523a9549b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1a69fd87ee14b5d962a9c8324455178":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c818f51c30114f77885fc352a3d910fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9165ae47ec44a2f8737415b28c3039b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ecf466e4d04712a0bcc95bc4319b8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c195588d5ac94c588698fcdc72af6ad6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5984b10c7fb44cb99cd962f3328eddb5":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"4b3991272ced4c76ab2d15f362e6b014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b938fd55a3224287a33f351e58c6d1fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c24e42c8e104c6fbc84bee0881bf5ef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b975b5de3a534bdd83836c875e31d1b1","IPY_MODEL_5a1001a0e0974b199fd785fffae3a85c","IPY_MODEL_bb69069067754791ab321a754dd22a40"],"layout":"IPY_MODEL_d6ec8516742941b2b36086302925df25"}},"b975b5de3a534bdd83836c875e31d1b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c33526217f94ec48878f81196f4fa0e","placeholder":"​","style":"IPY_MODEL_cf891bc3b73d4b758a433bd05ee75f3b","value":"Map:  96%"}},"5a1001a0e0974b199fd785fffae3a85c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfc51726ebcd4200bbdcfaa6f065ac1c","max":32262,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71d75514807e424582ddd83d62e9bbf8","value":32262}},"bb69069067754791ab321a754dd22a40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fdfbd63b9924c0cac865c9e25c411e1","placeholder":"​","style":"IPY_MODEL_0e0ae0cc04f0405bbc68a765eac79b86","value":" 31000/32262 [00:01&lt;00:00, 12340.78 examples/s]"}},"d6ec8516742941b2b36086302925df25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4c33526217f94ec48878f81196f4fa0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf891bc3b73d4b758a433bd05ee75f3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfc51726ebcd4200bbdcfaa6f065ac1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71d75514807e424582ddd83d62e9bbf8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fdfbd63b9924c0cac865c9e25c411e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e0ae0cc04f0405bbc68a765eac79b86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}